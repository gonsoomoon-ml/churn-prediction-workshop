{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.0] Inferencde Pipeline 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 노트북에서는 아래의 내용을 진행 합니다.\n",
    "    - Feature Transfomer(전처리 학습 모델) 생성\n",
    "    - Train 데이타를 Feature Transfomer를 통해서 전처리 데이타 생성\n",
    "    - Validation 데이타를 Feature Transfomer를 통해서 전처리 데이타 생성\n",
    "    - XGBoost를 학습\n",
    "    - Post-Processing Model 생성\n",
    "    - Inference Pipeline 생성\n",
    "- 소요 시간은 약 10분 걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformer (전처리 학습 모델) - preprocessing.py 파일\n",
    "- Numerical 데이타는 <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>StandardScaler</a>를 사용하여 Normalization을 함. \n",
    "    * z = (x - u) / s \n",
    "        * (z: 표준화된 값. 이 값을 학습시에 사용, x: 각 테이타의 값, u: 데이타 항목의 평균, s: 데이타 항목의 표준편차)\n",
    "- 아래 Account Length, ..CustServ Calls까지 모두 위의 방법으로 전처리 함.\n",
    "    - 아래 imputer는 결측값이 있을 경우에 해당 컬럼의 median 값을 사용 함.\n",
    "\n",
    "```python\n",
    "    numeric_features = list([\n",
    "    'Account Length',\n",
    "    'VMail Message',\n",
    "    'Day Mins',\n",
    "    'Day Calls',\n",
    "    'Eve Mins',\n",
    "    'Eve Calls',\n",
    "    'Night Mins',\n",
    "    'Night Calls',\n",
    "    'Intl Mins',\n",
    "    'Intl Calls',\n",
    "    'CustServ Calls'])\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "```\n",
    "- Categorical 데이타는 One Hot Encoding 방식으로 전처리 함. (예, 남자:0, 여자:1 일 경우에 남자:(1,0), 여자:(0,1) 방식으로 처리)\n",
    "    - State, Area Code, Int'l Plan, VMail Plan을 적용 함\n",
    "```python\n",
    "    categorical_features = ['State','Area Code',\"Int'l Plan\",'VMail Plan']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "```\n",
    "- 최종적으로 Numerical and Categorical Transformer를 합쳐서 Transformer 생성하고, 학습하여 Transformer의 모델을 S3에 업로드 함.\n",
    "```python\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)],\n",
    "        remainder=\"drop\")\n",
    "\n",
    "    preprocessor.fit(concat_data)\n",
    "\n",
    "    joblib.dump(preprocessor, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "```\n",
    "- Phone 데이타 항목은 위의 전처리 항목에서 제외 함. 유저별로 고유한 번호이기에 피쳐로서 의미가 없을 것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from time import strftime, gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformer (전처리 학습 모델) 생성\n",
    "아래는 다음과 같은 작업을 합니다.\n",
    "- SKLearn 이라는 Estimator를 생성 합니다. \n",
    "    - s3_input_train의 학습 데이타를 SKLearn 입력으로 제공 합니다.\n",
    "    - \"전처리 학습 모델 (Featurizer)\" 을 생성할 수 있는 소스 코드 preprocessing.py 를 지정 합니다. \n",
    "    - 사용할 리소스로 instance_type = 'local' 를 지정 합니다. (이미 노트북 인스턴스에 설치된 Docker-compose를 이용 합니다.)\n",
    "        - **Local 이 아니라 SageMaker Cloud Instance도 사용 가능 합니다. (예: ml.m4.xlarge)**\n",
    "        - **아래 XGBoost 학습 알고리즘을 사용시에는 SageMaker Cloud Instance 사용함**\n",
    "- SKLearn의 \"전처리 학습 모델\"이 완료가 되면 결과인 모델 아티펙트 파일이 (model.tar.gz)  s3://{bucket_name}/{job_name}/output.tar.gz 에 저장 됩니다. \n",
    "    - (예: s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-07-15-08-39-41-035/model.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래는 약 1분 정도가 소요 됩니다. 아래 셀의 [*] 의 표시가 [숫자] (에: [3])로 바뀔 때까지 기다려 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is not the latest supported version. If you would like to use version 0.23-1, please add framework_version=0.23-1 to your constructor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmphxbdmdkq_algo-1-6f0de_1 ... \n",
      "\u001b[1BAttaching to tmphxbdmdkq_algo-1-6f0de_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,118 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,120 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,129 sagemaker_sklearn_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,235 sagemaker-containers INFO     Module train-preprocessing does not provide a setup.py. \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,235 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,236 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:55,236 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m /miniconda3/bin/python -m pip install . \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Building wheels for collected packages: train-preprocessing\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m   Building wheel for train-preprocessing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \u001b[?25h  Created wheel for train-preprocessing: filename=train_preprocessing-1.0.0-py2.py3-none-any.whl size=9936 sha256=c1341224fdf98f837bafb646498201fa4ee33a888dd5301a4dc873671eb6078a\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-heqdpm0l/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Successfully built train-preprocessing\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Installing collected packages: train-preprocessing\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Successfully installed train-preprocessing-1.0.0\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:56,163 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:56,173 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"current_host\": \"algo-1-6f0de\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         \"algo-1-6f0de\"\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"hyperparameters\": {},\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m             \"TrainingInputMode\": \"File\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m             \"ContentType\": \"csv\"\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"job_name\": \"sagemaker-scikit-learn-2020-08-09-08-00-53-139\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"master_hostname\": \"algo-1-6f0de\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-08-09-08-00-53-139/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"module_name\": \"train-preprocessing\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"num_cpus\": 16,\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         \"current_host\": \"algo-1-6f0de\",\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m             \"algo-1-6f0de\"\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m     \"user_entry_point\": \"train-preprocessing.py\"\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_HOSTS=[\"algo-1-6f0de\"]\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_HPS={}\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_USER_ENTRY_POINT=train-preprocessing.py\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-6f0de\",\"hosts\":[\"algo-1-6f0de\"]}\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_CHANNELS=[\"train\"]\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_CURRENT_HOST=algo-1-6f0de\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_MODULE_NAME=train-preprocessing\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_NUM_CPUS=16\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-08-09-08-00-53-139/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-6f0de\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1-6f0de\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-08-09-08-00-53-139\",\"log_level\":20,\"master_hostname\":\"algo-1-6f0de\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-08-09-08-00-53-139/source/sourcedir.tar.gz\",\"module_name\":\"train-preprocessing\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-6f0de\",\"hosts\":[\"algo-1-6f0de\"]},\"user_entry_point\":\"train-preprocessing.py\"}\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_USER_ARGS=[]\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m PYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m /miniconda3/bin/python -m train-preprocessing\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m \n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m   import imp\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m saved model!\n",
      "\u001b[36malgo-1-6f0de_1  |\u001b[0m 2020-08-09 08:00:57,013 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmphxbdmdkq_algo-1-6f0de_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "sagemaker_session = sagemaker.Session()\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "script_path = 'train-preprocessing.py'\n",
    "# instance_type = 'ml.m4.2xlarge'\n",
    "instance_type = 'local'\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    train_instance_type = instance_type\n",
    ")\n",
    "sklearn_preprocessor.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transfomer를 사용하여 전처리된 학습 및 검증 데이타 생성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer_Train](img/Fig2.1.transformer_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Training data (Feature) 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래는 약 1분 정도가 소요 됩니다. 아래 셀의 [*] 의 표시가 [숫자] (에: [4])로 바뀔 때까지 기다려 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpinyftg_t_algo-1-hos2v_1\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m Building wheels for collected packages: train-preprocessing\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m   Building wheel for train-preprocessing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m \u001b[?25h  Created wheel for train-preprocessing: filename=train_preprocessing-1.0.0-py2.py3-none-any.whl size=9935 sha256=2330f06b019d3a8cc298ecf047af867af9ea8c5ab14c8a7de944aa73553bcc2f\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-074cd5ih/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m Successfully built train-preprocessing\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m Installing collected packages: train-preprocessing\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m Successfully installed train-preprocessing-1.0.0\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m   import imp\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [72] [INFO] Starting gunicorn 19.9.0\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [72] [INFO] Listening at: unix:/tmp/gunicorn.sock (72)\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [72] [INFO] Using worker: gevent\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [75] [INFO] Booting worker with pid: 75\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [91] [INFO] Booting worker with pid: 91\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [92] [INFO] Booting worker with pid: 92\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [123] [INFO] Booting worker with pid: 123\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [139] [INFO] Booting worker with pid: 139\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [141] [INFO] Booting worker with pid: 141\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [157] [INFO] Booting worker with pid: 157\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [173] [INFO] Booting worker with pid: 173\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [189] [INFO] Booting worker with pid: 189\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [190] [INFO] Booting worker with pid: 190\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [208] [INFO] Booting worker with pid: 208\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [209] [INFO] Booting worker with pid: 209\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [225] [INFO] Booting worker with pid: 225\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [241] [INFO] Booting worker with pid: 241\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:07 +0000] [273] [INFO] Booting worker with pid: 273\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m [2020-08-09 08:01:08 +0000] [304] [INFO] Booting worker with pid: 304\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m 2020-08-09 08:01:09,071 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m   import imp\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m 172.18.0.1 - - [09/Aug/2020:08:01:09 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m 172.18.0.1 - - [09/Aug/2020:08:01:09 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"-\"\n",
      "\u001b[36malgo-1-hos2v_1  |\u001b[0m 172.18.0.1 - - [09/Aug/2020:08:01:09 +0000] \"POST /invocations HTTP/1.1\" 200 1054525 \"-\" \"-\"\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "Waiting for transform job: sagemaker-scikit-learn-2020-08-09-08-00-2020-08-09-08-01-03-918\n",
      ".s3://sagemaker-us-east-2-057716757052/sagemaker/customer-churn/transformtrain-train-output/sagemaker-scikit-learn-2020-08-09-08-00-2020-08-09-08-01-03-918\n"
     ]
    }
   ],
   "source": [
    "# 아웃풋 경로 지정\n",
    "transform_train_output_path = 's3://{}/{}/{}/'.format(bucket, prefix, 'transformtrain-train-output')\n",
    "instance_type = 'local'\n",
    "# instance_type = 'ml.m4.2xlarge'\n",
    "\n",
    "# scikit_learn_inferencee_model 이름으로 전처리 학습 모델 생성\n",
    "# TRANSFORM_MODE의 환경 변수는 전처리 모드라는 것을 알려 줌.\n",
    "    # 추론시에는 환경 변수를 TRANSFORM_MODE\": \"inverse-label-transform\" 설정 함.\n",
    "    # 위의 두개의 과정을 분리할 수 있으나, 한개의 소스를 (preprocessor.py)를 사용하기 위해서, 환경 변수를 통해서 구분함.\n",
    "scikit_learn_inferencee_model = sklearn_preprocessor.create_model(\n",
    "    env={'TRANSFORM_MODE': 'feature-transform'})\n",
    "# scikit_learn_inferencee_model 에서 Train Transformer 생성\n",
    "transformer_train = scikit_learn_inferencee_model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type= instance_type,\n",
    "    assemble_with = 'Line',\n",
    "    output_path = transform_train_output_path,\n",
    "    accept = 'text/csv')\n",
    "\n",
    "\n",
    "# Preprocess training input\n",
    "transformer_train.transform(s3_input_train.config['DataSource']['S3DataSource']['S3Uri'], \n",
    "                            content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer_train.latest_transform_job.job_name)\n",
    "transformer_train.wait()\n",
    "preprocessed_train_path = transformer_train.output_path + transformer_train.latest_transform_job.job_name\n",
    "print(preprocessed_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training 전처리된 학습 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-057716757052/sagemaker/customer-churn/transformtrain-train-output/sagemaker-scikit-learn-2020-08-09-08-00-2020-08-09-08-01-03-918\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-09 08:01:10    1054526 sagemaker/customer-churn/transformtrain-train-output/sagemaker-scikit-learn-2020-08-09-08-00-2020-08-09-08-01-03-918/train.csv.out\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {preprocessed_train_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.11941369588439606</th>\n",
       "      <th>-0.5962380254245051</th>\n",
       "      <th>1.744368057672484</th>\n",
       "      <th>0.9789570533336895</th>\n",
       "      <th>-0.028992907038264654</th>\n",
       "      <th>-0.8931854019845896</th>\n",
       "      <th>-0.8017032037830547</th>\n",
       "      <th>-1.9825286353116254</th>\n",
       "      <th>-1.5305589315744583</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0.48</th>\n",
       "      <th>0.0.49</th>\n",
       "      <th>0.0.50</th>\n",
       "      <th>0.0.51</th>\n",
       "      <th>0.0.52</th>\n",
       "      <th>0.0.53</th>\n",
       "      <th>1.0.1</th>\n",
       "      <th>0.0.54</th>\n",
       "      <th>1.0.2</th>\n",
       "      <th>0.0.55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.852652</td>\n",
       "      <td>-0.596238</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.310405</td>\n",
       "      <td>0.970689</td>\n",
       "      <td>-0.689888</td>\n",
       "      <td>0.146389</td>\n",
       "      <td>1.232901</td>\n",
       "      <td>0.124852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.181295</td>\n",
       "      <td>-0.596238</td>\n",
       "      <td>1.835130</td>\n",
       "      <td>0.185503</td>\n",
       "      <td>0.030988</td>\n",
       "      <td>-0.639063</td>\n",
       "      <td>1.568529</td>\n",
       "      <td>-0.063643</td>\n",
       "      <td>-0.846802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776769</td>\n",
       "      <td>-0.596238</td>\n",
       "      <td>0.216227</td>\n",
       "      <td>0.334276</td>\n",
       "      <td>0.136954</td>\n",
       "      <td>1.393914</td>\n",
       "      <td>1.394712</td>\n",
       "      <td>-0.634123</td>\n",
       "      <td>0.844596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.234547</td>\n",
       "      <td>1.508734</td>\n",
       "      <td>-0.459859</td>\n",
       "      <td>0.483049</td>\n",
       "      <td>-0.230929</td>\n",
       "      <td>0.224952</td>\n",
       "      <td>1.056954</td>\n",
       "      <td>0.921730</td>\n",
       "      <td>-0.810815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751486</td>\n",
       "      <td>1.218393</td>\n",
       "      <td>0.231046</td>\n",
       "      <td>-0.756723</td>\n",
       "      <td>0.516833</td>\n",
       "      <td>0.275776</td>\n",
       "      <td>1.043127</td>\n",
       "      <td>-2.138114</td>\n",
       "      <td>0.232814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0  0.11941369588439606  -0.5962380254245051  1.744368057672484  \\\n",
       "0  0.0            -1.852652            -0.596238           0.140284   \n",
       "1  1.0             1.181295            -0.596238           1.835130   \n",
       "2  0.0             0.776769            -0.596238           0.216227   \n",
       "3  0.0            -0.234547             1.508734          -0.459859   \n",
       "4  0.0             0.751486             1.218393           0.231046   \n",
       "\n",
       "   0.9789570533336895  -0.028992907038264654  -0.8931854019845896  \\\n",
       "0           -0.310405               0.970689            -0.689888   \n",
       "1            0.185503               0.030988            -0.639063   \n",
       "2            0.334276               0.136954             1.393914   \n",
       "3            0.483049              -0.230929             0.224952   \n",
       "4           -0.756723               0.516833             0.275776   \n",
       "\n",
       "   -0.8017032037830547  -1.9825286353116254  -1.5305589315744583  ...  0.0.48  \\\n",
       "0             0.146389             1.232901             0.124852  ...     0.0   \n",
       "1             1.568529            -0.063643            -0.846802  ...     0.0   \n",
       "2             1.394712            -0.634123             0.844596  ...     0.0   \n",
       "3             1.056954             0.921730            -0.810815  ...     0.0   \n",
       "4             1.043127            -2.138114             0.232814  ...     0.0   \n",
       "\n",
       "   0.0.49  0.0.50  0.0.51  0.0.52  0.0.53  1.0.1  0.0.54  1.0.2  0.0.55  \n",
       "0     0.0     1.0     0.0     0.0     0.0    1.0     0.0    1.0     0.0  \n",
       "1     0.0     0.0     0.0     0.0     0.0    1.0     0.0    1.0     0.0  \n",
       "2     0.0     0.0     0.0     0.0     0.0    1.0     0.0    1.0     0.0  \n",
       "3     0.0     0.0     0.0     0.0     0.0    1.0     0.0    0.0     1.0  \n",
       "4     0.0     0.0     0.0     0.0     0.0    1.0     0.0    0.0     1.0  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_path_file = os.path.join (preprocessed_train_path, 'train.csv.out')\n",
    "df_pre_train = pd.read_csv(preprocessed_train_path_file)\n",
    "df_pre_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Validation data (Feature) 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래는 약 1분 정도가 소요 됩니다. 아래 셀의 [*] 의 표시가 [숫자] (에: [8])로 바뀔 때까지 기다려 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아웃풋 경로 지정\n",
    "transform_validation_output_path = 's3://{}/{}/{}/'.format(bucket, prefix, 'transformtrain-validation-output')\n",
    "# scikit_learn_inferencee_model 에서 Validation Transformer 생성\n",
    "transformer_validation = scikit_learn_inferencee_model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type= instance_type,\n",
    "    assemble_with = 'Line',\n",
    "    output_path = transform_validation_output_path,\n",
    "    accept = 'text/csv')\n",
    "# Preprocess validation input\n",
    "transformer_validation.transform(s3_input_validation.config['DataSource']['S3DataSource']['S3Uri'], content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer_validation.latest_transform_job.job_name)\n",
    "transformer_validation.wait()\n",
    "preprocessed_validation_path = transformer_validation.output_path+transformer_validation.latest_transform_job.job_name\n",
    "print(preprocessed_validation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_validation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {preprocessed_validation_path} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train with XGBoost on SageMaker Cloud Instance\n",
    "아 과정은 위의 전처리된 데이타를 가지고 실제 SageMaker Built-in Algorithm XGBoost를 이용하여 학습을 수행 합니다.<br>\n",
    "실제의 학습 과정은 SageMaker Cloud Instance에서 실제 학습니 됩니다.\n",
    "\n",
    "\n",
    "Built-in XGboost 알고리즘 컨테이너를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3에 있는 Train, Validation 전처리된 (Features) 데이타의 경로 및 파일 포맷등을 지정하는 오브젝트를 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train_processed = sagemaker.session.s3_input(\n",
    "    preprocessed_train_path, \n",
    "    distribution='FullyReplicated',\n",
    "    content_type='text/csv', \n",
    "    s3_data_type='S3Prefix')\n",
    "print(\"S3 Train input: \\n\")\n",
    "print(s3_input_train_processed.config)\n",
    "s3_input_validation_processed = sagemaker.session.s3_input(\n",
    "    preprocessed_validation_path, \n",
    "    distribution='FullyReplicated',\n",
    "    content_type='text/csv', \n",
    "    s3_data_type='S3Prefix')\n",
    "print(\"\\nS3 Validation input: \\n\")\n",
    "print(s3_input_validation_processed.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래는 약 5분 정도가 소요 됩니다. 아래 셀의 [*] 의 표시가 [숫자] (에: [13])로 바뀔 때까지 기다려 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "instance_type = 'ml.m4.2xlarge'\n",
    "\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container, # Built-in XGBoost Container\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type= instance_type,\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess\n",
    "                                   )\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                       )\n",
    "\n",
    "\n",
    "xgb.fit({'train': s3_input_train_processed, 'validation': s3_input_validation_processed}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "- XGBoost에서 나온 결과 값( 0 <= Score <=1) 을 0.5 이하이면 False, 이상이면 True로 변환 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn_preprocessor 로 부터 후처리 모델을 생성 함.(scikit_learn_post_process_model)\n",
    "# 다만 환경 변수를 바꾸어 후처리용으로 사용 함 ()'TRANSFORM_MODE': 'inverse-label-transform')\n",
    "# transform_postprocessor_path = 's3://{}/{}/{}/'.format(bucket, prefix, 'transformtrain-postprocessing-output')\n",
    "\n",
    "# transformer_post_processing = scikit_learn_post_process_model.transformer(\n",
    "#     instance_count=1, \n",
    "#     instance_type='local',\n",
    "#     assemble_with = 'Line',\n",
    "#     output_path = transform_postprocessor_path,\n",
    "#     accept = 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline <a class=\"anchor\" id=\"pipeline_setup\"></a>\n",
    "\n",
    "아래 그림과 같이 위에서 생성한 전처리, 알고리즘 학습, 후처리의 세가지 모델을 가지고 1개의 단일 모델을 만들어 Inference Pipleline을 생성 합니다. <br>\n",
    "**입력 데이타 가공이 없이 실제 데이타가 입력이 되면, 1개의 단일 모델을 통해서 최종적으로 예측 결과인 True, False의 결과 값이 제공 됩니다.**\n",
    "\n",
    "![Inference-pipeline](img/Fig2.2.inference_pipeline.png)\n",
    "\n",
    "\n",
    "**Machine Learning Model Pipeline (Inference Pipeline)는 create_model() 를 호출하여 만들 수 있습니다.** <br>\n",
    "예를 들어 여기서는 the fitted Scikit-learn inference model, the fitted Xgboost model and the psotprocessing model 의 세가지 모델을 가지고 만듦니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 세개 모델을 생성함. 전처리, 후처리 모델 생성시에는 환경 변수를 제공 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 모델\n",
    "scikit_learn_pre_process_model = sklearn_preprocessor.create_model(\n",
    "    env={'TRANSFORM_MODE': 'feature-transform'})    \n",
    "# 학습 모델\n",
    "xgb_model = xgb.create_model()\n",
    "# 후처리 모델\n",
    "scikit_learn_post_process_model = sklearn_preprocessor.create_model(\n",
    "    env={'TRANSFORM_MODE': 'inverse-label-transform'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Transformer Model:\\n {}\".format(scikit_learn_pre_process_model.model_data))\n",
    "print(\"\\nXGBoost Model:\\n {}\".format(xgb_model.model_data))\n",
    "print(\"\\nPost-Processing Model :\\n {}\".format(scikit_learn_post_process_model.model_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 모델의 기타 설정 변수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entry_point: \", scikit_learn_pre_process_model.entry_point)\n",
    "print(\"framework_version: \", scikit_learn_pre_process_model.framework_version)\n",
    "print(\"env: \", scikit_learn_pre_process_model.env)\n",
    "print(\"model_data: \", scikit_learn_pre_process_model.model_data)\n",
    "print(\"name: \", scikit_learn_pre_process_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime('$Y-%m-%d-%H-%M-%S', gmtime())\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_name = 'churn-model-inference-pipeline-' + timestamp_prefix\n",
    "\n",
    "\n",
    "sklearn_preprocessor.create_model\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name = model_name,\n",
    "    role = role,\n",
    "    models = [\n",
    "        scikit_learn_pre_process_model,\n",
    "        xgb_model,\n",
    "        scikit_learn_post_process_model        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "instance_type='ml.t2.medium'\n",
    "endpoint_name= 'churn-model-pipeline-endpoint-' + timestamp_prefix\n",
    "\n",
    "\n",
    "\n",
    "pipeline_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type= instance_type, \n",
    "    endpoint_name = endpoint_name    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint = endpoint_name,\n",
    "    sagemaker_session = sagemaker_session,\n",
    "    serializer = csv_serializer,\n",
    "    content_type = CONTENT_TYPE_CSV,\n",
    "    accept = CONTENT_TYPE_JSON\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_inference_format(sample):\n",
    "    instance = str()\n",
    "    for i, token in enumerate(sample):\n",
    "        # print(token)\n",
    "        if i > 0:\n",
    "            instance = instance  + ',' + str(token) \n",
    "        else:\n",
    "            instance = instance  +  str(token) \n",
    "    return instance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../churn_data/batch_transform_test.csv\", header=None)\n",
    "\n",
    "for i in range(10):\n",
    "    sample = test_df.iloc[i,:]\n",
    "    instance = make_inference_format(sample)\n",
    "    print(\"instance: \\n\", instance)\n",
    "\n",
    "    payload = instance\n",
    "    churn_result = predictor.predict(payload)\n",
    "    print(\"Churn result?: \\n\", churn_result)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
